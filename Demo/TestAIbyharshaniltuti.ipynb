{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "AI-Powered Climate Change Monitoring Demo\n",
    "\n",
    "Based on the project description: AI-Powered Climate Change Monitoring and Mitigation System.\n",
    "This Colab notebook demonstrates core concepts using live data:\n",
    "1. Data Acquisition (Weather API)\n",
    "2. Data Visualization\n",
    "3. Short-Term Forecasting (Temperature Prediction)\n",
    "4. Anomaly Detection\n",
    "\"\"\"\n",
    "\n",
    "# @title 1. Project Setup and Dependencies\n",
    "# Install necessary libraries (if not already included in Colab)\n",
    "!pip install requests pandas numpy matplotlib seaborn scikit-learn tensorflow plotly kaleido folium statsmodels --quiet\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import folium\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time # For potential API rate limiting\n",
    "\n",
    "print(\"Libraries installed and imported.\")\n",
    "\n",
    "# @title 2. Configuration\n",
    "# --- Parameters for Data Acquisition ---\n",
    "\n",
    "# Using Open-Meteo API (free, no key required for basic use)\n",
    "# Reference: https://open-meteo.com/\n",
    "\n",
    "# Coordinates for a specific location (e.g., London, UK)\n",
    "LATITUDE = 51.5074\n",
    "LONGITUDE = -0.1278\n",
    "LOCATION_NAME = \"London, UK\"\n",
    "\n",
    "# Weather variables to fetch\n",
    "# Options: temperature_2m, relativehumidity_2m, apparent_temperature, precipitation, rain, snowfall,\n",
    "#          weathercode, surface_pressure, cloudcover, et0_fao_evapotranspiration, vapor_pressure_deficit,\n",
    "#          windspeed_10m, winddirection_10m, windgusts_10m, soil_temperature_0_to_7cm, soil_moisture_0_to_7cm etc.\n",
    "# Also possible: air quality variables like pm2_5, carbon_monoxide, nitrogen_dioxide, sulphur_dioxide, ozone (needs air_quality model in API call)\n",
    "# Check API docs for availability. We'll start simple.\n",
    "HOURLY_VARIABLES = [\"temperature_2m\", \"precipitation\", \"relativehumidity_2m\"]\n",
    "\n",
    "# How many past days of data to fetch for analysis and training\n",
    "PAST_DAYS = 90 # Fetch ~3 months of recent historical data\n",
    "\n",
    "# --- Parameters for Models ---\n",
    "N_STEPS_LSTM = 5  # Number of time steps to look back for LSTM prediction\n",
    "N_FEATURES_LSTM = 1 # We will predict temperature based on past temperature\n",
    "\n",
    "ANOMALY_CONTAMINATION = 0.02 # Expected proportion of outliers in the data for Isolation Forest\n",
    "\n",
    "\n",
    "# @title 3. Data Acquisition (Live API Call)\n",
    "\n",
    "def fetch_climate_data(latitude, longitude, past_days, hourly_vars):\n",
    "    \"\"\"Fetches historical weather data from Open-Meteo API.\"\"\"\n",
    "    print(f\"Fetching data for coordinates: ({latitude}, {longitude})\")\n",
    "    BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\" # Use archive API for recent past\n",
    "\n",
    "    # Calculate date range\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    start_date = (datetime.now() - timedelta(days=past_days)).strftime('%Y-%m-%d')\n",
    "\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": \",\".join(hourly_vars),\n",
    "        \"timezone\": \"auto\" # Use local timezone of the coordinates\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        response.raise_for_status() # Raise HTTPError for bad responses (4XX or 5XX)\n",
    "        data = response.json()\n",
    "        print(\"Data fetched successfully!\")\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON response.\")\n",
    "        print(\"Response text:\", response.text) # Print response if it's not valid JSON\n",
    "        return None\n",
    "\n",
    "# Fetch the data\n",
    "raw_data = fetch_climate_data(LATITUDE, LONGITUDE, PAST_DAYS, HOURLY_VARIABLES)\n",
    "\n",
    "# Process the data if fetched successfully\n",
    "if raw_data:\n",
    "    hourly_data = raw_data.get('hourly', {})\n",
    "    df = pd.DataFrame(hourly_data)\n",
    "\n",
    "    if 'time' not in df.columns or df.empty:\n",
    "         print(\"Error: 'time' column not found in hourly data or data is empty.\")\n",
    "         print(\"Raw Data Structure:\", raw_data) # Helps debug API response issues\n",
    "         df = pd.DataFrame() # Set df to empty to avoid downstream errors\n",
    "    else:\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        print(f\"\\nDataframe created with shape: {df.shape}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nLast 5 rows:\")\n",
    "        print(df.tail())\n",
    "        print(\"\\nData Info:\")\n",
    "        df.info()\n",
    "else:\n",
    "    print(\"Could not proceed without data.\")\n",
    "    # Assign an empty dataframe to prevent errors in subsequent cells if execution continues\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# @title 4. Data Preprocessing and Cleaning\n",
    "\n",
    "if not df.empty:\n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Simple imputation: Forward fill for time series data\n",
    "    # More sophisticated methods could be used (interpolation, model-based imputation)\n",
    "    df.ffill(inplace=True) # Forward fill handles missing sensor readings potentially\n",
    "    df.bfill(inplace=True) # Back fill handles any remaining NaNs at the beginning\n",
    "\n",
    "    print(\"\\nMissing values after forward/backward fill:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Ensure numeric types (sometimes API might return strings if errors occur)\n",
    "    for col in HOURLY_VARIABLES:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce') # Coerce converts errors to NaT/NaN\n",
    "\n",
    "    # Re-check and drop rows if any NaNs persisted after coercion (shouldn't happen with ffill/bfill unless all data for a col was bad)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    print(f\"\\nDataframe shape after cleaning: {df.shape}\")\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe())\n",
    "else:\n",
    "    print(\"Skipping preprocessing as DataFrame is empty.\")\n",
    "\n",
    "\n",
    "# @title 5. Exploratory Data Analysis (EDA) & Visualization\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"\\n--- Generating Visualizations ---\")\n",
    "\n",
    "    # --- 5.1 Time Series Plots (Interactive with Plotly) ---\n",
    "    print(\"\\nPlotting Time Series...\")\n",
    "    fig_ts = go.Figure()\n",
    "\n",
    "    for col in HOURLY_VARIABLES:\n",
    "        fig_ts.add_trace(go.Scatter(x=df.index, y=df[col], mode='lines', name=col))\n",
    "\n",
    "    fig_ts.update_layout(\n",
    "        title=f'Hourly Climate Variables for {LOCATION_NAME} (Last {PAST_DAYS} Days)',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Value',\n",
    "        legend_title='Variable',\n",
    "        hovermode=\"x unified\" # Nice hover effect\n",
    "    )\n",
    "    fig_ts.show()\n",
    "\n",
    "    # --- 5.2 Distribution Plots (Histograms using Plotly Express) ---\n",
    "    print(\"\\nPlotting Distributions...\")\n",
    "    for col in HOURLY_VARIABLES:\n",
    "      fig_hist = px.histogram(df, x=col, title=f'Distribution of {col}')\n",
    "      fig_hist.show()\n",
    "\n",
    "    # --- 5.3 Location Map (using Folium) ---\n",
    "    print(\"\\nGenerating Location Map...\")\n",
    "    m = folium.Map(location=[LATITUDE, LONGITUDE], zoom_start=10)\n",
    "    folium.Marker(\n",
    "        [LATITUDE, LONGITUDE],\n",
    "        popup=f\"{LOCATION_NAME}\\nLat: {LATITUDE}\\nLon: {LONGITUDE}\",\n",
    "        tooltip=LOCATION_NAME\n",
    "    ).add_to(m)\n",
    "    display(m) # Display map in Colab output\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Visualization as DataFrame is empty.\")\n",
    "\n",
    "\n",
    "# @title 6. Climate Prediction Model (Short-Term Temperature Forecast - LSTM)\n",
    "\n",
    "if not df.empty and 'temperature_2m' in df.columns and len(df) > N_STEPS_LSTM * 2: # Ensure enough data for train/test split\n",
    "    print(\"\\n--- Building Short-Term Temperature Forecast Model (LSTM) ---\")\n",
    "\n",
    "    # Select Temperature data and scale it\n",
    "    temp_data = df['temperature_2m'].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(temp_data)\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    def create_sequences(data, n_steps):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data)):\n",
    "            end_ix = i + n_steps\n",
    "            if end_ix > len(data)-1:\n",
    "                break\n",
    "            seq_x, seq_y = data[i:end_ix], data[end_ix]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X, y = create_sequences(scaled_data, N_STEPS_LSTM)\n",
    "\n",
    "    # Reshape for LSTM [samples, timesteps, features]\n",
    "    X = X.reshape((X.shape[0], X.shape[1], N_FEATURES_LSTM))\n",
    "\n",
    "    # Split data (using first 80% for training, rest for testing)\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    time_index_test = df.index[N_STEPS_LSTM:][split_idx:] # Keep track of the timestamps for plotting test results\n",
    "\n",
    "    print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"Testing data shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "    # Build the LSTM Model\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(50, activation='relu', input_shape=(N_STEPS_LSTM, N_FEATURES_LSTM)))\n",
    "    # model_lstm.add(LSTM(50, activation='relu')) # Optional second layer\n",
    "    model_lstm.add(Dense(1))\n",
    "    model_lstm.compile(optimizer='adam', loss='mse') # Mean Squared Error loss\n",
    "\n",
    "    # Train the Model\n",
    "    print(\"\\nTraining LSTM model...\")\n",
    "    # Increase epochs for better performance, but keep low for demo speed\n",
    "    history = model_lstm.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=0) # verbose=0 to keep output clean\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('LSTM Model Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Make Predictions\n",
    "    y_pred_scaled = model_lstm.predict(X_test)\n",
    "    y_pred = scaler.inverse_transform(y_pred_scaled) # Inverse scale to get actual temperature values\n",
    "    y_test_actual = scaler.inverse_transform(y_test) # Inverse scale the true values for comparison\n",
    "\n",
    "    # Visualize Predictions vs Actual\n",
    "    print(\"\\nPlotting Forecast vs Actual...\")\n",
    "    fig_pred = go.Figure()\n",
    "    fig_pred.add_trace(go.Scatter(x=time_index_test, y=y_test_actual.flatten(), mode='lines', name='Actual Temperature'))\n",
    "    fig_pred.add_trace(go.Scatter(x=time_index_test, y=y_pred.flatten(), mode='lines', name='Predicted Temperature (LSTM)'))\n",
    "\n",
    "    fig_pred.update_layout(\n",
    "        title=f'Temperature Forecast vs Actual for {LOCATION_NAME} (Test Set)',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Temperature (°C)',\n",
    "        legend_title='Data',\n",
    "         hovermode=\"x unified\"\n",
    "    )\n",
    "    fig_pred.show()\n",
    "\n",
    "    # Calculate some simple error metric (e.g., Mean Absolute Error)\n",
    "    mae = np.mean(np.abs(y_pred - y_test_actual))\n",
    "    print(f\"\\nMean Absolute Error (MAE) on Test Set: {mae:.2f} °C\")\n",
    "    print(\"\\nNote: This is a simple short-term forecast based only on recent temperature history.\")\n",
    "    print(\"Real climate prediction requires much more data and complex models.\")\n",
    "\n",
    "else:\n",
    "     print(\"Skipping Prediction: DataFrame is empty, 'temperature_2m' column is missing, or not enough data points.\")\n",
    "\n",
    "# @title 7. Anomaly Detection (using Isolation Forest)\n",
    "\n",
    "if not df.empty and 'temperature_2m' in df.columns:\n",
    "    print(\"\\n--- Performing Anomaly Detection on Temperature Data ---\")\n",
    "\n",
    "    # Using Isolation Forest - good for finding outliers\n",
    "    model_iforest = IsolationForest(n_estimators=100, contamination=ANOMALY_CONTAMINATION, random_state=42)\n",
    "\n",
    "    # Fit on the temperature data (or could use multiple features)\n",
    "    temp_data_if = df[['temperature_2m']].copy() # Select data, ensure it's a DataFrame\n",
    "    model_iforest.fit(temp_data_if)\n",
    "\n",
    "    # Predict outliers: -1 for anomalies, 1 for inliers\n",
    "    df['anomaly_temp'] = model_iforest.predict(temp_data_if)\n",
    "\n",
    "    # Extract anomalies\n",
    "    anomalies = df[df['anomaly_temp'] == -1]\n",
    "    print(f\"\\nDetected {len(anomalies)} potential anomalies in temperature data.\")\n",
    "\n",
    "    # Visualize Anomalies\n",
    "    print(\"\\nPlotting Temperature with Detected Anomalies...\")\n",
    "    fig_anomaly = go.Figure()\n",
    "\n",
    "    # Plot all temperature data\n",
    "    fig_anomaly.add_trace(go.Scatter(\n",
    "        x=df.index,\n",
    "        y=df['temperature_2m'],\n",
    "        mode='lines',\n",
    "        name='Temperature (°C)'\n",
    "    ))\n",
    "\n",
    "    # Add markers for anomalies\n",
    "    fig_anomaly.add_trace(go.Scatter(\n",
    "        x=anomalies.index,\n",
    "        y=anomalies['temperature_2m'],\n",
    "        mode='markers',\n",
    "        marker=dict(color='red', size=8, symbol='x'),\n",
    "        name='Detected Anomaly'\n",
    "    ))\n",
    "\n",
    "    fig_anomaly.update_layout(\n",
    "        title=f'Temperature Time Series with Anomaly Detection for {LOCATION_NAME}',\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Temperature (°C)',\n",
    "        legend_title='Data',\n",
    "         hovermode=\"x unified\"\n",
    "    )\n",
    "    fig_anomaly.show()\n",
    "\n",
    "    if not anomalies.empty:\n",
    "      print(\"\\nDetails of detected anomalies:\")\n",
    "      print(anomalies[['temperature_2m', 'anomaly_temp']])\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Anomaly Detection: DataFrame is empty or 'temperature_2m' column is missing.\")\n",
    "\n",
    "\n",
    "# @title 8. Simple Dashboard / Summary Visualization\n",
    "\n",
    "# This section combines key plots into a simple overview.\n",
    "# In a real application, this would use dashboarding tools like Dash/Streamlit, Power BI, Tableau.\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"\\n--- Summary Dashboard Components ---\")\n",
    "\n",
    "    # Re-displaying key plots generated above for a \"dashboard-like\" summary in Colab\n",
    "    print(\"\\n1. Time Series Overview:\")\n",
    "    if 'fig_ts' in locals(): # Check if plot was generated\n",
    "      fig_ts.show()\n",
    "    else:\n",
    "      print(\"Time series plot not available.\")\n",
    "\n",
    "\n",
    "    print(\"\\n2. Temperature Forecast (Test Set):\")\n",
    "    if 'fig_pred' in locals():\n",
    "        fig_pred.show()\n",
    "    else:\n",
    "      print(\"Forecast plot not available.\")\n",
    "\n",
    "    print(\"\\n3. Temperature Anomaly Detection:\")\n",
    "    if 'fig_anomaly' in locals():\n",
    "        fig_anomaly.show()\n",
    "    else:\n",
    "      print(\"Anomaly plot not available.\")\n",
    "\n",
    "\n",
    "    print(\"\\n4. Location Context:\")\n",
    "    if 'm' in locals():\n",
    "      display(m)\n",
    "    else:\n",
    "      print(\"Map not available.\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot generate summary dashboard components as DataFrame is empty.\")\n",
    "\n",
    "\n",
    "# @title 9. Conclusion & Future Work\n",
    "\n",
    "print(\"\\n--- Conclusion & Future Work ---\")\n",
    "print(\"This Colab notebook demonstrated a basic workflow for AI-powered climate monitoring:\")\n",
    "print(\"1. Fetched near real-time hourly weather data using the Open-Meteo API.\")\n",
    "print(\"2. Cleaned and visualized the time series data (temperature, precipitation, humidity).\")\n",
    "print(\"3. Implemented a short-term temperature forecast using an LSTM model.\")\n",
    "print(\"4. Applied Isolation Forest to detect anomalies in the temperature data.\")\n",
    "\n",
    "print(\"\\nThis is a simplified demonstration based on the comprehensive system described in 'project 2.txt'.\")\n",
    "\n",
    "print(\"\\nPotential Future Enhancements (Closer to the Full Vision):\")\n",
    "print(\"- **Integrate More Data Sources:** Add satellite imagery (e.g., NDVI, land surface temperature via Google Earth Engine API), CO2 concentration data (e.g., MethaneSAT, NOAA), sea level data, etc.\")\n",
    "print(\"- **Advanced Prediction Models:** Utilize more complex models like CNNs, RNNs (Transformers) on larger, multi-variate datasets for more accurate and longer-term climate event predictions (requires significant historical data).\")\n",
    "print(\"- **Sophisticated Anomaly Detection:** Explore multi-variate anomaly detection and algorithms robust to seasonality (e.g., Autoencoders, Prophet).\")\n",
    "print(\"- **Impact Visualization:** Develop dynamic, interactive dashboards using tools like Plotly Dash, Streamlit, Tableau, or Power BI for policymakers and public access.\")\n",
    "print(\"- **Geospatial Analysis:** Perform deeper analysis using GIS tools, correlating climate data with geographic features (deforestation, urban sprawl).\")\n",
    "print(\"- **Mitigation Simulation (Advanced):** Carefully design simulation environments to test policy impacts using Reinforcement Learning or Agent-Based Modeling (complex research area).\")\n",
    "print(\"- **Scalability & Deployment:** Move from Colab to a scalable cloud platform (AWS, GCP, Azure) using Big Data tools (Spark, Dask) and MLOps practices for continuous integration and model updates.\")\n",
    "print(\"- **Regional Customization:** Adapt models and dashboards for specific regions and climate challenges.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
